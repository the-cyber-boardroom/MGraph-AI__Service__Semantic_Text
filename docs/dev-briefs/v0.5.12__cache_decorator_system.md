# Cache Decorator System

A transparent, decorator-based caching system for the Semantic Text Service that automatically stores and retrieves method responses using the MGraph AI Cache Service.

## Overview

The cache decorator system provides a simple way to add caching to any method without polluting the business logic. It uses the `@cache_response` decorator to transparently cache method results based on configurable rules.

## Architecture

### Core Components

1. **Semantic_Text__Cache** - Thin wrapper around the cache service client
2. **Cache__Config** - Type-safe configuration for cache behavior
3. **Cache__Decorator** - Main `@cache_response` decorator implementation
4. **Cache__Key__Generator** - Generates cache keys from method parameters
5. **Enum__Cache__Decorator__Mode** - Operating modes (ENABLED, DISABLED, READ_ONLY)

### Storage Strategy

The system uses the `KEY_BASED` storage strategy with the following path structure:

```
namespace/data/key-based/{class_name}/{method_name}/{param_hash}/response.json
```

Example:
```
semantic-text/transformations/data/key-based/
  Text__Transformation__Service/transform/abc123def4/response.json
```

## Quick Start

### 1. Setup Cache Client

```python
from mgraph_ai_service_semantic_text.cache import Semantic_Text__Cache

# Initialize cache client (typically in service setup)
cache = Semantic_Text__Cache().setup()
```

### 2. Configure Cache Behavior

```python
from mgraph_ai_service_semantic_text.cache import Cache__Config

# Create cache configuration
CACHE_CONFIG__MY_SERVICE = Cache__Config(
    namespace    = "semantic-text/my-service",
    key_fields   = ["param1", "param2"],  # Which params to use for cache key
    file_id      = "response"
)
```

### 3. Apply Decorator to Methods

```python
from mgraph_ai_service_semantic_text.cache import cache_response

class My_Service(Type_Safe):
    semantic_text__cache : Semantic_Text__Cache = None  # Injected during setup
    
    @cache_response(CACHE_CONFIG__MY_SERVICE)
    def my_method(self, param1: str, param2: int) -> Response:
        # Method logic here - caching happens transparently
        ...
```

## Complete Example

```python
from osbot_utils.type_safe.Type_Safe                                                import Type_Safe
from mgraph_ai_service_semantic_text.cache                                          import Semantic_Text__Cache, cache_response
from mgraph_ai_service_semantic_text.cache.Cache__Configs                           import CACHE_CONFIG__TEXT_TRANSFORMATION
from mgraph_ai_service_semantic_text.schemas.transformation.Schema__Text__Transformation__Request   import Schema__Text__Transformation__Request
from mgraph_ai_service_semantic_text.schemas.transformation.Schema__Text__Transformation__Response  import Schema__Text__Transformation__Response


class Text__Transformation__Service(Type_Safe):
    semantic_text__cache : Semantic_Text__Cache = None  # Injected during setup
    
    def setup(self):
        # Initialize cache client
        self.semantic_text__cache = Semantic_Text__Cache().setup()
        # ... other setup
        return self
    
    @cache_response(CACHE_CONFIG__TEXT_TRANSFORMATION)
    def transform(self, 
                  request: Schema__Text__Transformation__Request
             ) -> Schema__Text__Transformation__Response:
        # Original transformation logic - caching is automatic
        engine = self._get_engine(request.transformation_mode)
        transformed_mapping = engine.transform(request.hash_mapping)
        
        return Schema__Text__Transformation__Response(
            transformed_mapping = transformed_mapping,
            transformation_mode = request.transformation_mode,
            success             = True,
            # ... other fields
        )
```

## Configuration Options

### Cache__Config Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `namespace` | `Safe_Str__Namespace` | Required | Cache namespace (e.g., "semantic-text/transformations") |
| `enabled` | `bool` | `True` | Master switch for caching |
| `mode` | `Enum__Cache__Decorator__Mode` | `ENABLED` | Operating mode (ENABLED, DISABLED, READ_ONLY) |
| `strategy` | `Enum__Cache__Store__Strategy` | `KEY_BASED` | Storage strategy |
| `key_fields` | `List[str]` | `[]` | Method param names to include in cache key |
| `use_class_name` | `bool` | `True` | Include class name in cache path |
| `use_method_name` | `bool` | `True` | Include method name in cache path |
| `file_id` | `str` | `"response"` | Fixed file_id for stored responses |
| `cache_attr_name` | `str` | `"semantic_text__cache"` | Attribute name for cache client |
| `ttl_seconds` | `Optional[Safe_UInt]` | `None` | Time to live (handled by cache service) |
| `cache_none_results` | `bool` | `False` | Whether to cache None results |
| `invalidate_on_error` | `bool` | `False` | Clear cache if method raises exception |

### Operating Modes

```python
from mgraph_ai_service_semantic_text.cache.enums import Enum__Cache__Decorator__Mode

# ENABLED - Normal operation (read and write)
config.mode = Enum__Cache__Decorator__Mode.ENABLED

# DISABLED - Bypass cache completely
config.mode = Enum__Cache__Decorator__Mode.DISABLED

# READ_ONLY - Only read from cache, never write
config.mode = Enum__Cache__Decorator__Mode.READ_ONLY
```

## Pre-defined Configurations

The system includes pre-defined configurations in `Cache__Configs.py`:

```python
from mgraph_ai_service_semantic_text.cache.Cache__Configs import (
    CACHE_CONFIG__TEXT_TRANSFORMATION,
    CACHE_CONFIG__SEMANTIC_CLASSIFICATION,
    CACHE_CONFIG__GENERAL
)

@cache_response(CACHE_CONFIG__TEXT_TRANSFORMATION)
def my_transformation_method(self, ...):
    ...
```

## How It Works

### Cache Key Generation

1. **Extract Parameters**: Based on `key_fields`, relevant parameters are extracted
2. **Serialize Values**: Type_Safe objects are converted to dicts, primitives stay as-is
3. **Generate Hash**: JSON serialize with sorted keys → MD5 hash → first 10 chars
4. **Build Path**: Combine class name, method name, and hash

Example:
```python
# For method: Text__Transformation__Service.transform(request)
# With key_fields = ["hash_mapping", "transformation_mode"]

Cache Key: Text__Transformation__Service/transform/a3f5b2c1d4
Full Path: semantic-text/transformations/data/key-based/
           Text__Transformation__Service/transform/a3f5b2c1d4/response.json
```

### Decorator Flow

```
1. Method called with parameters
   ↓
2. Decorator intercepts call
   ↓
3. Check if caching is enabled/configured
   ↓
4. Generate cache_key from parameters
   ↓
5. Try to retrieve from cache
   ↓
   ├─ Cache HIT → Deserialize and return
   │
   └─ Cache MISS → Execute method
                   ↓
                   Store result in cache
                   ↓
                   Return result
```

### Type-Safe Roundtrip

The decorator automatically handles Type_Safe object serialization:

```python
# Storing
response_object.json()  # → JSON string

# Retrieving
ResponseClass.from_json(cached_json)  # → Type_Safe object
```

## Advanced Usage

### Dynamic Cache Configuration

Pass configuration at runtime:

```python
@cache_response(config_param="cache_config")
def my_method(self, data: str, cache_config: Cache__Config = None):
    # If cache_config is passed, use it; otherwise skip caching
    ...
```

### Custom Cache Key Logic

For complex scenarios, create custom configurations:

```python
CACHE_CONFIG__CUSTOM = Cache__Config(
    namespace      = "semantic-text/custom"        ,
    key_fields     = ["url", "mode", "timestamp"]  ,  # Multiple fields
    use_class_name = False                         ,  # Flat namespace
    file_id        = "result"                      ,
)
```

### Disable Caching Temporarily

```python
# Via environment variable (recommended for testing)
os.environ['CACHE_ENABLED'] = 'false'

# Via config
config.enabled = False

# Via mode
config.mode = Enum__Cache__Decorator__Mode.DISABLED
```

## Testing

### Mocking Cache Behavior

```python
def test_with_cache():
    service = Text__Transformation__Service()
    service.semantic_text__cache = Mock()  # Mock cache client
    
    # Test with cache
    result = service.transform(request)
    assert service.semantic_text__cache.cache_client.retrieve.called

def test_without_cache():
    service = Text__Transformation__Service()
    # Don't set semantic_text__cache
    
    # Should work without cache
    result = service.transform(request)
```

### Testing Cache Hits/Misses

```python
def test_cache_hit():
    service = Text__Transformation__Service().setup()
    
    # First call - cache miss
    result1 = service.transform(request)
    
    # Second call - cache hit (same params)
    result2 = service.transform(request)
    
    assert result1.json() == result2.json()
```

## Best Practices

1. **Always inject cache client**: Set `semantic_text__cache` during service setup
2. **Use pre-defined configs**: Start with `Cache__Configs.py` configurations
3. **Choose key_fields carefully**: Only include fields that affect the result
4. **Keep namespaces organized**: Use hierarchical structure (e.g., "semantic-text/transformations")
5. **Test without cache first**: Ensure methods work correctly before adding caching
6. **Monitor cache stats**: Use cache service admin endpoints to track performance

## Troubleshooting

### Cache Not Working

1. Check if `semantic_text__cache` is set on the instance
2. Verify cache service environment variables are configured
3. Check `config.enabled` is `True`
4. Look for warning logs about cache client

### Cache Key Collisions

1. Ensure `key_fields` includes all parameters that affect the result
2. Consider adding `use_class_name=True` to avoid cross-class collisions

### Type Serialization Issues

1. Ensure response types have `.json()` and `.from_json()` methods
2. Check that nested Type_Safe objects serialize correctly
3. Use `.obj()` for debugging serialization

## Performance Considerations

- **Cache overhead**: ~5-20ms for cache lookup (fast!)
- **Network vs compute**: Cache is beneficial when method execution > 50ms
- **Memory usage**: KEY_BASED strategy uses cloud storage (not local memory)
- **Serverless**: Cache persists across Lambda invocations

## Environment Variables

Required for cache service connection:

```bash
export ENV_VAR__URL__TARGET_SERVER__CACHE_SERVICE="https://cache.dev.mgraph.ai"
export ENV_VAR__AUTH__TARGET_SERVER__CACHE_SERVICE__KEY_NAME="X-API-Key"
export ENV_VAR__AUTH__TARGET_SERVER__CACHE_SERVICE__KEY_VALUE="your-api-key"
```

## Future Enhancements

- [ ] Async method support (`@cache_response` for `async def`)
- [ ] Cache warming strategies
- [ ] Automatic cache invalidation on data changes
- [ ] Cache metrics and monitoring integration
- [ ] Distributed cache locking for concurrent writes